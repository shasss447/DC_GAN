# DCGAN from Scratch Using PyTorch

This project implements the **Deep Convolutional Generative Adversarial Network (DCGAN)** from the research paper *"Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks"* by Alec Radford, Luke Metz, and Soumith Chintala. The model is built from scratch in PyTorch, following the architectural guidelines outlined in the paper to achieve high-quality image generation on the CIFAR-10 dataset.

## Project Overview

This project is a PyTorch implementation of the DCGAN model as described in the original research paper. The **Generator** and **Discriminator** architectures are designed according to the paper’s specifications, utilizing strided convolutions and batch normalization in a deep convolutional network. The model is trained on CIFAR-10, and the training process includes tracking the generator and discriminator losses to monitor GAN performance.
Highlights:
- Implemented in PyTorch following the DCGAN research paper’s architectural guidelines.
- Training is conducted on the CIFAR-10 dataset with resized images to fit the DCGAN framework.

## Dataset

The model is trained on the **CIFAR-10** dataset, a widely used dataset of 60,000 32x32 color images across 10 classes. The images are resized to 64x64 pixels to align with the DCGAN model requirements.

## Model Architecture

# Generator
The **Generator** (`G`) is responsible for transforming a 100-dimensional latent vector `z` (sampled from a standard normal distribution) into a 64x64 RGB image. Following the DCGAN paper's guidelines, `G` uses a series of transposed convolutions to progressively upsample `z`, with batch normalization and ReLU activations at each layer.
Key architectural elements:
- **Input**: 100-dimensional latent vector `z`.
**Upsampling Layers**: Transposed convolutional layers increase the spatial resolution from 4x4 to 64x64, following the DCGAN design for efficient upsampling.
- **Output Layer**: The last transposed convolution outputs a 3x64x64 RGB image, with a Tanh activation function to normalize pixel values between -1 and 1.

# Discriminator
The Discriminator (`D`) follows the DCGAN architecture’s deep convolutional framework, which downsamples images from 64x64 down to a single probability value indicating the likelihood of an image being real. Each layer employs strided convolutions, LeakyReLU activations, and omits any pooling layers to preserve feature information.
Key architectural elements:
- **Input**: 3x64x64 RGB image.
**Upsampling Layers**: Successive convolutional layers with strided convolutions to reduce spatial dimensions.
- **Output Layer**: A final fully connected layer outputs a single probability value through a Sigmoid activation.

## Training
The training setup adheres to the DCGAN paper’s specifications:
- **Loss Function**: Binary Cross Entropy Loss (BCELoss) is used for both `G` and `D`.
- **Optimizers**: Adam optimizers are used with a learning rate of `0.0002` and `β1=0.5`, as recommended by the paper.
- **Training Loop**: The generator and discriminator losses are computed and logged at each step. Images generated by the model are saved periodically for visual inspection.

## Results

### Generator and Discriminator Losses
Below is the graph showing the loss trends for both the generator and discriminator during training.
![](/Loss_compare.png)
### Real vs. Fake Images
This image shows a comparison between real CIFAR-10 images and images generated by the DCGAN model:
![](/real_fake.png)

## References
- ["Radford, A., Metz, L., & Chintala, S. (2015)"](https://arxiv.org/pdf/1511.06434)